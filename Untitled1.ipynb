{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "import sklearn.svm\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neighbors\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saket/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/saket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(docs):\n",
    "    new_docs = []\n",
    "    i= 0 \n",
    "    for document in docs:\n",
    "        raw = document.lower()\n",
    "        raw = raw.replace(\"<br /><br />\", \" \")\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        lemmed_tokens = [(wordnet_lemmatizer.lemmatize(token[0]),token[1]) for token in pos]\n",
    "        new_docs.append(lemmed_tokens)\n",
    "        if i%5000 == 0:\n",
    "            print i\n",
    "        i += 1\n",
    "    with open('../dataset/lemmed_dev.pkl', 'wb') as f:\n",
    "        pickle.dump(new_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../dataset/audio_dev.json' \n",
    "documents = []\n",
    "class_labels = []\n",
    "summary = []\n",
    "\n",
    "with open(filepath,'r') as fp:  \n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        input_data = (json.loads(line))\n",
    "        documents.append(input_data[\"reviewText\"])\n",
    "        summary.append(input_data[\"summary\"])\n",
    "        line = fp.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "cleaning(documents[0:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning2(sample,file_name,adjective):\n",
    "    file_name = '../dataset/' + file_name\n",
    "    with open(file_name, 'rb') as f:\n",
    "        lemmed_tokens = pickle.load(f)\n",
    "    new_docs = []\n",
    "    for i in range(min(sample,len(lemmed_tokens))):\n",
    "        token_list = lemmed_tokens[i]\n",
    "        if adjective:\n",
    "            adj_list = [tag[0] for tag in token_list if tag[1] == 'JJ']\n",
    "        stopped_tokens = [token[0] for token in token_list if token[0] not in en_stop]\n",
    "        if adjective:\n",
    "            stopped_tokens = stopped_tokens + adj_list\n",
    "        documentWords = ' '.join(stopped_tokens)\n",
    "        new_docs.append(documentWords)\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legendary hard rock metal band still cold kickin hardcore live hit target major way release music critic time sounded alarm one quite frankly missed boat opinion ashamedly passed one back day read one many poor review even though dug title cut wa played radio allmusic ha given album one half star lowest rated review beg differ cat reviewed missed boat believe state killer opening cut contagious barely disguised rehash bon jovi livin prayer chorus like whoa oa oa simply nothing like perenial wimp bon jovi lesser quality wimp fest tune keyboard dense bon jovi cut flowery high pitched dig good keyboard hard rock want deep powerful tinkly fey reviewer also called sleek overproduced 80 pop metal question doe kick certainly doe care slick sound probably care 100 similar lesser band era album probably wa tearin way like bon jovi crue ilk content album 5 star album low point course cliched token lyric plagued band vein boy night breakin chain anyone however album simply put sound amazing one finest sounding mix collection want call slick every piece puzzle mesh perfect place dave meniketti guitar shread like rock guitar mixed 10 ton lead ripping speaker limit razor sharp hear string hanging dear life joey alves add guitar rip mix well resulting crunch pitch perfect nobody mix like anymore shame late phil kennemore bass pop thud amazingly newcomer jimmy degrasso thunderous drum mic perfection course personal preference soundwise lp tear major way keyboard perfect well nothing flouncy obtrusive add overall feel check eye stranger hear element sync guitar edge bass popping drum pounding pop metal get much better sound design blow away man last certainly least dave meniketti vocal always spot one greatest vocalist genere said live new line 2012 nothing slamming meniketti voice still amazing ever guitar playing simply top notch surrounded real nice group musician much like seen phil k jimmy new guy get done big way check recent cd facemelter support team prop newest version prop classic discovered year initial release check sample see become one old school faves well legendary hard cold live major many poor i y contagious disguised other perenial flowery high good hard deep powerful pop t other similar isn low cliched other perfect s other mixed sharp dear pitch perfect shame late s newcomer thunderous mic personal major perfect flouncy overall sync last dave s new s amazing top real nice much i phil new big recent classic few initial old\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
